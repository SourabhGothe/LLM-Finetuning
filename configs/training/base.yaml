# Base Training Configuration

per_device_train_batch_size: 2
gradient_accumulation_steps: 4
warmup_steps: 50
num_train_epochs: 3
learning_rate: 2e-5  # Using the stabilized learning rate
fp16: false
bf16: true
logging_steps: 1
optim: "adamw_8bit"
weight_decay: 0.01
lr_scheduler_type: "linear"
seed: 42
