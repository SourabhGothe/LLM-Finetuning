# Experiment: Mistral-7B-Instruct-v0.3 with QLoRA on WebNLG

defaults:
  - base_config

# Override specific parameters for this experiment
run_name: "mistral_7b_instruct_qlora_webnlg_3_epochs_${now:%Y-%m-%d_%H-%M-%S}"

model:
  name: "unsloth/mistral-7b-instruct-v0.3-bnb-4bit"
  name_sanitized: "mistral-7b-instruct-v0-3" # A shorter name for clean directory paths

# You can adjust training parameters here if needed
training:
  num_train_epochs: 3
  learning_rate: 0.0001 # 1e-4

peft:
  r: 16 # Mistral models often perform well with a rank of 8 or 16
  lora_alpha: 32
